{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSAC9kzWrDQN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\", \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\", \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
        "\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "metadata": {
        "id": "pdREfhZurmof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "for s, l in train_dataset:\n",
        "  sentences.append(s.numpy().decode('utf8'))\n",
        "  labels.append(l.numpy())\n",
        "\n",
        "print(f\" จำนวนประโยคทั้งหมด: {len(sentences)} ประโยค\")\n",
        "print(\"ตัวอย่างประโยคแรก:\", sentences[0])\n",
        "print(\"Label ของประโยคนี้ (0=Neg, 1=Pos):\", labels[0])\n",
        "\n",
        "print(f\"จำนวนคลาสทั้งหมด: {info.features['label'].num_classes}\")\n",
        "\n",
        "\n",
        "print(f\"ชื่อของแต่ละคลาส: {info.features['label'].names}\")"
      ],
      "metadata": {
        "id": "kqBu_eNvrnAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0:2]\n"
      ],
      "metadata": {
        "id": "s9tekxcKtlHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = []\n",
        "\n",
        "\n",
        "if len(sentences) > 0:\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        soup = BeautifulSoup(sentence, \"html.parser\")\n",
        "        sentence = soup.get_text()\n",
        "\n",
        "\n",
        "        sentence = sentence.translate(table)\n",
        "\n",
        "        # แยกคำและลบ Stopwords\n",
        "        words = sentence.split()\n",
        "        filtered_sentence = [w for w in words if w not in stopwords]\n",
        "\n",
        "        cleaned_sentences.append(\" \".join(filtered_sentence))\n",
        "\n",
        "    print(\"Clean Data Completely!\")\n",
        "    print(\"ตัวอย่างประโยคก่อนคลีน:\", sentences[0])\n",
        "    print(\"ตัวอย่างประโยคหลังคลีน:\", cleaned_sentences[0])\n",
        "else:\n",
        "    print(\"Error: ไม่มีข้อมูลในลิสต์ sentences กรุณากลับไปเช็คขั้นตอนการโหลดข้อมูล\")"
      ],
      "metadata": {
        "id": "1mVPlu4Qsah1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ow8JchkDDzWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "sentence_lengths = [len(s.split()) for s in cleaned_sentences]\n",
        "\n",
        "\n",
        "plt.hist(sentence_lengths, bins=50)\n",
        "plt.xlabel('Length of Sentence')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "print(f\"Mean length: {np.mean(sentence_lengths)}\")\n",
        "print(f\"Median length: {np.median(sentence_lengths)}\")\n",
        "print(f\"90th percentile: {np.percentile(sentence_lengths, 90)}\")"
      ],
      "metadata": {
        "id": "SUNx2MRdrpG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 2000\n",
        "embedding_dim = 7\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 24000\n"
      ],
      "metadata": {
        "id": "djwr9kmPu8t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences = cleaned_sentences[0:training_size]\n",
        "testing_sentences = cleaned_sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "metadata": {
        "id": "SH8aVZ4YvVt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "PC4AUm47vDXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences[1]"
      ],
      "metadata": {
        "id": "pM4Ay65P5r74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc=tokenizer.word_counts\n",
        "print(wc)"
      ],
      "metadata": {
        "id": "Vr98Mo69vZw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "wc = tokenizer.word_counts\n",
        "from collections import OrderedDict\n",
        "newlist = (OrderedDict(sorted(wc.items(), key=lambda t: t[1], reverse=True)))\n",
        "print(word_index)\n",
        "# print(newlist)\n",
        "xs=[]\n",
        "ys=[]\n",
        "curr_x = 1\n",
        "for item in newlist:\n",
        "  xs.append(curr_x)\n",
        "  curr_x=curr_x+1\n",
        "  ys.append(newlist[item])\n",
        "\n",
        "print(ys)\n",
        "plt.plot(xs, ys)\n",
        "plt.xlabel(\"Word Rank\")\n",
        "plt.ylabel(\"Word Frequency\")\n",
        "plt.title(\"Word Frequency Distribution of IMDB Reviews\")\n",
        "plt.show()\n",
        "\n",
        "print(ys[1000])\n",
        "print(ys[2000])"
      ],
      "metadata": {
        "id": "TgwQb8wLvd_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "-OjfUX-2xweW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.1)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "M_ft3q0Yx9Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "RxwXJzyryCfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    training_padded,\n",
        "    training_labels,\n",
        "    epochs=150,\n",
        "    validation_data=(testing_padded, testing_labels),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZzwLLNxayE3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "OGBr4AVc0ol1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_sentence(training_padded[2]))\n",
        "print(training_sentences[2])\n",
        "print(labels[2])"
      ],
      "metadata": {
        "id": "XU3RVc5E0rU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n"
      ],
      "metadata": {
        "id": "9J0-saGu0tgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_word_index[2])\n",
        "print(weights[2])"
      ],
      "metadata": {
        "id": "cTd_-Xfw0wjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecssen.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metasen.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecssen.tsv')\n",
        "  files.download('metasen.tsv')"
      ],
      "metadata": {
        "id": "iZsxeFiQ1Icp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"I absolutely loved this movie! The acting was superb and the story was very touching.\",\n",
        "    \"This was the worst film I have ever seen. A complete waste of time and money.\",\n",
        "    \"The cinematography was beautiful, but the plot was boring and the characters were flat.\",\n",
        "    \"I thought it would be a good movie, but it turned out to be very disappointing and slow.\",\n",
        "    \"An incredible masterpiece. I will definitely watch it again and recommend it to everyone!\"\n",
        "]\n",
        "sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "predictions = model.predict(padded)\n",
        "\n",
        "for s, p in zip(test_sentences, predictions):\n",
        "    print(f\"{p[0]:.3f} = {s}\")\n"
      ],
      "metadata": {
        "id": "TVEoj-SW4UGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}