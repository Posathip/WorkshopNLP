{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYYDvoskkE61"
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpOdgZnNlWY5"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
        "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
        "             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
        "             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
        "             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
        "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
        "             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n",
        "             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n",
        "             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n",
        "             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n",
        "             \"yourselves\"]\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQVuQrZNkPn9"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaLaaqhNkUPd"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "print(\"จำนวนข้อมูลทั้งหมด:\", len(datastore))\n",
        "print(\"\\nKeys ที่มีในแต่ละ record:\")\n",
        "print(datastore[0].keys())\n",
        "\n",
        "print(\"\\nตัวอย่าง record แรก (raw):\")\n",
        "for k, v in datastore[1].items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "datastore[:100]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentence = item['headline'].lower()\n",
        "    sentence = sentence.replace(\",\", \" , \")\n",
        "    sentence = sentence.replace(\".\", \" . \")\n",
        "    sentence = sentence.replace(\"-\", \" - \")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "    words = sentence.split()\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word + \" \"\n",
        "    sentences.append(filtered_sentence)\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])"
      ],
      "metadata": {
        "id": "VyVSYltyv1RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class 0 : NOT Sarcasm\")\n",
        "count = 0\n",
        "for s, l in zip(sentences, labels):\n",
        "    if l == 0:\n",
        "        print(\"-\", s)\n",
        "        count += 1\n",
        "        if count == 10:\n",
        "            break\n",
        "\n",
        "print(\"\\nClass 1 : Sarcasm\")\n",
        "count = 0\n",
        "for s, l in zip(sentences, labels):\n",
        "    if l == 1:\n",
        "        print(\"-\", s)\n",
        "        count += 1\n",
        "        if count == 10:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "YAMAUVN-pelk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "all_words = []\n",
        "for sentence in sentences:\n",
        "    all_words.extend(sentence.split())\n",
        "\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "total_unique_words = len(word_counts)\n",
        "\n",
        "print(f\"1. จำนวนคำศัพท์ที่ไม่ซ้ำกันทั้งหมด (Total Unique Words): {total_unique_words} คำ\")"
      ],
      "metadata": {
        "id": "srokk9pmEYaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shYL7Vg8QVfb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "sentence_lengths = [len(s.split()) for s in sentences]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.hist(sentence_lengths, bins=50, color='teal', edgecolor='white')\n",
        "\n",
        "plt.title(\"Histogram of Sarcram Lengths (Word Count)\", fontsize=14)\n",
        "plt.xlabel(\"Number of Words\", fontsize=12)\n",
        "plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f\"ค่าเฉลี่ย (Mean): {np.mean(sentence_lengths):.2f} คำ\")\n",
        "print(f\"ค่ากลาง (Median): {np.median(sentence_lengths)} คำ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eJSTTYnkJQd"
      },
      "source": [
        "vocab_size = 2000\n",
        "embedding_dim = 7\n",
        "max_length = 20\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 24000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1sD-7v0kYWk"
      },
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u8UB0MCkZ5N"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_padded[2]"
      ],
      "metadata": {
        "id": "xw4p3OBpDe-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slS_cvYg0CgO"
      },
      "source": [
        "wc=tokenizer.word_counts\n",
        "print(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHWzYWl7N6HH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "wc = tokenizer.word_counts\n",
        "from collections import OrderedDict\n",
        "newlist = (OrderedDict(sorted(wc.items(), key=lambda t: t[1], reverse=True)))\n",
        "print(word_index)\n",
        "print(newlist)\n",
        "xs=[]\n",
        "ys=[]\n",
        "curr_x = 1\n",
        "for item in newlist:\n",
        "  xs.append(curr_x)\n",
        "  curr_x=curr_x+1\n",
        "  ys.append(newlist[item])\n",
        "\n",
        "print(ys)\n",
        "plt.plot(xs,ys)\n",
        "#plt.axis([300,10000,0,100])\n",
        "plt.show()\n",
        "print(ys[1000])\n",
        "print(ys[2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDk2X5LF5hwI"
      },
      "source": [
        "print(ys[3125])\n",
        "print(ys[10000])\n",
        "print(ys[12156])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrAlWBKf99Ya"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FufaT4vlkiDE"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.1)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#     tf.keras.layers.Dense(24, activation='relu'),\n",
        "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "# adam = tf.keras.optimizers.Adam(learning_rate=0.000005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "# model.compile(loss='binary_crossentropy',optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfDt1hmYkiys"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DTKQFf1kkyc"
      },
      "source": [
        "num_epochs = 150\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    training_padded,\n",
        "    training_labels,\n",
        "    epochs=150,\n",
        "    validation_data=(testing_padded, testing_labels),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# acc = accuracy_score(y_true, y_pred)\n",
        "# precision = precision_score(y_true, y_pred)\n",
        "# recall = recall_score(y_true, y_pred)\n",
        "# f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# print(f\"Accuracy  : {acc:.4f}\")\n",
        "# print(f\"Precision : {precision:.4f}\")\n",
        "# print(f\"Recall    : {recall:.4f}\")\n",
        "# print(f\"F1-score  : {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "cCxmi-1Yt4Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYfBKXjkmU8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SBdAZAenvzL"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_sentence(training_padded[2]))\n",
        "print(training_sentences[2])\n",
        "print(labels[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MqihtEkzQ9"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x79hCiBscirR"
      },
      "source": [
        "print(reverse_word_index[2])\n",
        "print(weights[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "metadata": {
        "id": "-lWcr1TrQ43Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8-ArY-qDcz"
      },
      "source": [
        "test_sentences = [\n",
        "    \"Oh great, another meeting that could have been an email.\",\n",
        "    \"It's a beautiful sunny day.\",\n",
        "    \"I just love it when my code crashes right before the deadline.\",\n",
        "    \"Yeah, because staying up all night is exactly how I wanted to spend my weekend.\",\n",
        "    \"Oh sure, I love waiting in line for two hours.\"\n",
        "]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "predictions = model.predict(padded)\n",
        "\n",
        "for s, p in zip(test_sentences, predictions):\n",
        "    print(f\"{p[0]:.3f} = {s}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz56oE8QNoNK"
      },
      "source": []
    }
  ]
}